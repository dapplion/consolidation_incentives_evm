//! Integration tests for the prover workspace
//!
//! These tests validate the end-to-end proof generation pipeline:
//! 1. Load test vectors generated by the test-vectors binary
//! 2. Verify proofs are correctly formatted and match expected values
//! 3. Cross-validate that Rust-generated proofs work with Solidity verification
//! 4. Test the full pipeline from state construction to proof bundle generation

use proof_gen::{ConsolidationProofBundle, GindexCalculator, ProofGenerator};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::PathBuf;

/// Test vector format matching contracts/test-vectors/test_vectors.json
#[derive(Debug, Serialize, Deserialize)]
struct TestVectors {
    preset: String,
    block_root: String,
    beacon_timestamp: u64,
    max_epoch: u64,
    claims: Vec<ValidClaim>,
    invalid_claims: Vec<InvalidClaim>,
}

#[derive(Debug, Serialize, Deserialize)]
struct ValidClaim {
    consolidation_index: u64,
    source_index: u64,
    activation_epoch: u64,
    source_credentials: String,
    proof_consolidation: Vec<String>,
    proof_credentials: Vec<String>,
    proof_activation_epoch: Vec<String>,
    expected_recipient: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct InvalidClaim {
    description: String,
    consolidation_index: u64,
    source_index: u64,
    activation_epoch: u64,
    source_credentials: String,
    proof_consolidation: Vec<String>,
    proof_credentials: Vec<String>,
    proof_activation_epoch: Vec<String>,
    expected_error: String,
}

fn load_test_vectors() -> TestVectors {
    let manifest_dir = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    // Go up from integration-tests/ -> crates/ -> prover/ -> workspace root
    let vectors_path = manifest_dir
        .parent() // integration-tests -> crates
        .expect("crates dir")
        .parent() // crates -> prover
        .expect("prover dir")
        .parent() // prover -> workspace root
        .expect("workspace root")
        .join("contracts")
        .join("test-vectors")
        .join("test_vectors.json");

    let content = fs::read_to_string(&vectors_path)
        .unwrap_or_else(|e| panic!("Failed to read test vectors from {:?}: {}", vectors_path, e));

    serde_json::from_str(&content)
        .unwrap_or_else(|e| panic!("Failed to parse test vectors: {}", e))
}

fn hex_to_bytes32(hex: &str) -> [u8; 32] {
    let hex = hex.strip_prefix("0x").unwrap_or(hex);
    let bytes = hex::decode(hex).expect("valid hex");
    let mut arr = [0u8; 32];
    arr.copy_from_slice(&bytes[..32]);
    arr
}

fn hex_to_vec_bytes32(hexes: &[String]) -> Vec<[u8; 32]> {
    hexes.iter().map(|h| hex_to_bytes32(h)).collect()
}

#[test]
fn test_load_test_vectors() {
    let vectors = load_test_vectors();
    assert_eq!(vectors.preset, "gnosis");
    assert!(!vectors.block_root.is_empty());
    assert!(vectors.beacon_timestamp > 0);
    assert_eq!(vectors.max_epoch, 1000);
    assert!(!vectors.claims.is_empty(), "Should have valid claims");
    assert!(
        !vectors.invalid_claims.is_empty(),
        "Should have invalid claims"
    );
}

#[test]
fn test_valid_claim_proof_lengths() {
    let vectors = load_test_vectors();

    let expected_consolidation_len = GindexCalculator::consolidation_proof_length() as usize;
    let expected_validator_len = GindexCalculator::validator_proof_length() as usize;

    for (i, claim) in vectors.claims.iter().enumerate() {
        assert_eq!(
            claim.proof_consolidation.len(),
            expected_consolidation_len,
            "Claim {} consolidation proof length mismatch",
            i
        );
        assert_eq!(
            claim.proof_credentials.len(),
            expected_validator_len,
            "Claim {} credentials proof length mismatch",
            i
        );
        assert_eq!(
            claim.proof_activation_epoch.len(),
            expected_validator_len,
            "Claim {} activation epoch proof length mismatch",
            i
        );
    }
}

#[test]
fn test_valid_claim_proof_format() {
    let vectors = load_test_vectors();

    for (i, claim) in vectors.claims.iter().enumerate() {
        // Verify all proofs are valid hex strings
        for proof_elem in claim.proof_consolidation.iter() {
            hex_to_bytes32(proof_elem); // Will panic if invalid
        }

        for proof_elem in claim.proof_credentials.iter() {
            hex_to_bytes32(proof_elem);
        }

        for proof_elem in claim.proof_activation_epoch.iter() {
            hex_to_bytes32(proof_elem);
        }

        // Verify credentials format
        let creds = hex_to_bytes32(&claim.source_credentials);
        assert!(
            creds[0] == 0x01 || creds[0] == 0x02,
            "Claim {} should have valid credential prefix (0x01 or 0x02), got 0x{:02x}",
            i,
            creds[0]
        );

        // Verify expected recipient can be parsed
        assert!(
            claim.expected_recipient.starts_with("0x"),
            "Claim {} recipient should start with 0x",
            i
        );
        assert_eq!(
            claim.expected_recipient.len(),
            42,
            "Claim {} recipient should be 42 chars (0x + 40 hex)",
            i
        );
    }
}

#[test]
fn test_invalid_claims_have_descriptions() {
    let vectors = load_test_vectors();

    for (i, claim) in vectors.invalid_claims.iter().enumerate() {
        assert!(
            !claim.description.is_empty(),
            "Invalid claim {} should have description",
            i
        );
        assert!(
            !claim.expected_error.is_empty(),
            "Invalid claim {} should have expected_error",
            i
        );
    }
}

#[test]
fn test_proof_lengths_match_gindex_depth() {
    // Verify that proof lengths match the computed gindex depths
    let consolidation_gindex = GindexCalculator::consolidation_source_gindex(0);
    let consolidation_depth = consolidation_gindex.ilog2();
    assert_eq!(
        consolidation_depth,
        GindexCalculator::consolidation_proof_length() as u32
    );

    let validator_gindex = GindexCalculator::validator_credentials_gindex(0);
    let validator_depth = validator_gindex.ilog2();
    assert_eq!(
        validator_depth,
        GindexCalculator::validator_proof_length() as u32
    );
}

#[test]
fn test_gindex_calculator_consistency() {
    // Verify gindex calculations are consistent across calls
    let gindex1 = GindexCalculator::consolidation_source_gindex(0);
    let gindex2 = GindexCalculator::consolidation_source_gindex(0);
    assert_eq!(gindex1, gindex2);

    // Different indices should give different gindices
    let gindex3 = GindexCalculator::consolidation_source_gindex(1);
    assert_ne!(gindex1, gindex3);
}

#[test]
fn test_gindex_depth_calculation() {
    // Verify that gindex depth matches expected proof lengths
    let consolidation_gindex = GindexCalculator::consolidation_source_gindex(0);
    let consolidation_depth = consolidation_gindex.ilog2(); // floor(log2(gindex))
    assert_eq!(
        consolidation_depth,
        GindexCalculator::consolidation_proof_length() as u32
    );

    let validator_creds_gindex = GindexCalculator::validator_credentials_gindex(0);
    let validator_depth = validator_creds_gindex.ilog2();
    assert_eq!(
        validator_depth,
        GindexCalculator::validator_proof_length() as u32
    );

    let validator_epoch_gindex = GindexCalculator::validator_activation_epoch_gindex(0);
    let validator_epoch_depth = validator_epoch_gindex.ilog2();
    assert_eq!(
        validator_epoch_depth,
        GindexCalculator::validator_proof_length() as u32
    );
}

#[test]
fn test_proof_bundle_serialization() {
    // Load test vectors and use the first claim to test serialization
    let vectors = load_test_vectors();
    let claim = &vectors.claims[0];

    let bundle = ConsolidationProofBundle {
        beacon_timestamp: vectors.beacon_timestamp,
        consolidation_index: claim.consolidation_index,
        source_index: claim.source_index,
        activation_epoch: claim.activation_epoch,
        source_credentials: hex_to_bytes32(&claim.source_credentials),
        proof_consolidation: hex_to_vec_bytes32(&claim.proof_consolidation),
        proof_credentials: hex_to_vec_bytes32(&claim.proof_credentials),
        proof_activation_epoch: hex_to_vec_bytes32(&claim.proof_activation_epoch),
    };

    // Serialize to JSON
    let json = serde_json::to_string(&bundle).expect("serialization");

    // Deserialize back
    let deserialized: ConsolidationProofBundle =
        serde_json::from_str(&json).expect("deserialization");

    // Verify fields match
    assert_eq!(deserialized.beacon_timestamp, bundle.beacon_timestamp);
    assert_eq!(
        deserialized.consolidation_index,
        bundle.consolidation_index
    );
    assert_eq!(deserialized.source_index, bundle.source_index);
    assert_eq!(deserialized.activation_epoch, bundle.activation_epoch);
    assert_eq!(
        deserialized.source_credentials,
        bundle.source_credentials
    );
    assert_eq!(
        deserialized.proof_consolidation.len(),
        bundle.proof_consolidation.len()
    );
    assert_eq!(
        deserialized.proof_credentials.len(),
        bundle.proof_credentials.len()
    );
    assert_eq!(
        deserialized.proof_activation_epoch.len(),
        bundle.proof_activation_epoch.len()
    );
}

#[test]
fn test_cross_validate_test_vectors_structure() {
    let vectors = load_test_vectors();

    // Verify all valid claims have consistent structure
    for (i, claim) in vectors.claims.iter().enumerate() {
        let creds = hex_to_bytes32(&claim.source_credentials);

        // Verify credentials are valid
        assert!(
            creds[0] == 0x01 || creds[0] == 0x02,
            "Claim {} should have valid credentials prefix",
            i
        );

        // Verify all proofs have expected lengths for gnosis preset
        let expected_consolidation_len = GindexCalculator::consolidation_proof_length() as usize;
        let expected_validator_len = GindexCalculator::validator_proof_length() as usize;

        assert_eq!(
            claim.proof_consolidation.len(),
            expected_consolidation_len,
            "Claim {} consolidation proof length",
            i
        );
        assert_eq!(
            claim.proof_credentials.len(),
            expected_validator_len,
            "Claim {} credentials proof length",
            i
        );
        assert_eq!(
            claim.proof_activation_epoch.len(),
            expected_validator_len,
            "Claim {} activation epoch proof length",
            i
        );
    }
}

#[test]
fn test_all_valid_claims_eligible() {
    let vectors = load_test_vectors();

    // All valid claims should have activation_epoch < max_epoch
    for (i, claim) in vectors.claims.iter().enumerate() {
        assert!(
            claim.activation_epoch < vectors.max_epoch,
            "Valid claim {} should be eligible (epoch {} < max {})",
            i,
            claim.activation_epoch,
            vectors.max_epoch
        );
    }
}

#[test]
fn test_invalid_claims_variety() {
    let vectors = load_test_vectors();

    // Verify we have a variety of invalid claim types
    let mut has_tampered_proof = false;
    let mut has_wrong_field = false;
    let mut has_ineligible = false;
    let mut has_bls = false;
    let mut has_swapped = false;

    for claim in &vectors.invalid_claims {
        if claim.description.contains("tampered") {
            has_tampered_proof = true;
        }
        if claim.description.contains("wrong") && 
           (claim.description.contains("source_index") || 
            claim.description.contains("credentials") || 
            claim.description.contains("activation_epoch")) {
            has_wrong_field = true;
        }
        if claim.description.contains("ineligible") || 
           claim.description.contains("equals maxEpoch") {
            has_ineligible = true;
        }
        if claim.description.contains("BLS") || claim.source_credentials.starts_with("0x00") {
            has_bls = true;
        }
        if claim.description.contains("swapped") {
            has_swapped = true;
        }
    }

    assert!(
        has_tampered_proof,
        "Should have tampered proof test cases"
    );
    assert!(has_wrong_field, "Should have wrong field test cases");
    assert!(has_ineligible, "Should have ineligible test cases");
    assert!(has_bls, "Should have BLS credentials test case");
    assert!(has_swapped, "Should have swapped proofs test case");
}

#[test]
fn test_gindex_increasing_for_sequential_indices() {
    // For sequential indices in the same list, gindices should increase
    // This validates the gindex computation follows the SSZ tree structure correctly

    let gindex0 = GindexCalculator::consolidation_source_gindex(0);
    let gindex1 = GindexCalculator::consolidation_source_gindex(1);
    let gindex2 = GindexCalculator::consolidation_source_gindex(2);

    // PendingConsolidation is a container with 2 fields (source_index, target_index)
    // Each container occupies a subtree of size 2 (next power of 2 >= 2 fields)
    // So gindices for source_index field jump by 2 between consecutive consolidations
    assert_eq!(gindex1, gindex0 + 2, "Consolidation gindices should differ by 2");
    assert_eq!(gindex2, gindex1 + 2, "Consolidation gindices should differ by 2");

    // Same for validators
    let val_gindex0 = GindexCalculator::validator_credentials_gindex(0);
    let val_gindex1 = GindexCalculator::validator_credentials_gindex(1);

    // Validators are containers with 8 fields (depth 3 subtree)
    // So gindices for a specific field jump by 8 between consecutive validators
    assert_eq!(val_gindex1, val_gindex0 + 8, "Validator gindices should differ by 8");
}
